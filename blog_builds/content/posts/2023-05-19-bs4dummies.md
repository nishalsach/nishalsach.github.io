---
title: "BeautifulSoup4Dummies"
date: 2023-05-19
draft: true
description: 
tags: [web-scraping, python, code]
---

**Note**: All links are from BS v4.12.0 documentation and reference, but the gist should probably hold for a while.

BeautifulSoup4 + Python is a formidable combo for web-scraping when the territory I'm scraping in is uncertain, webpages are static, and I need to spin up something quick and dirty so I can hunt around through the HTML to specifically extract what I need. 

Another thing is: when pulling web data with BS4, sometimes it's really handy to keep separate files of the subset/tags/text in a page that are of interest, but then also save original HTML files just to have them on hand in case extraction does not go according to plan.

Through this scraping and cleaning process, there are some coniderations involved:

### Scraping: Choice of Parser

Parsers are important to specify because [each parser behaves a bit differently](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers), especially if webpages are formatted wonkily (which is decently often) and your work needs to be reproducible.

Parser choices include: Python's native ```html.parser```, C-based ```lxml``` parsers, and finally HTML5. ```lxml``` is the quickest and most efficient, but has external dependencies. ```html.parser``` has no dependencies but being Python native, it's slower. It's also less lenient in what webpage inaccuracies/wonkiness it will permit. I find it best to use ```lxml```.

```python
# Imports
from bs4 import BeautifulSoup, Tag, NavigableString
import requests

# Query the website and return the html
req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})
webpage = urlopen(req).read()

# Parse the html in the 'webpage' variable, and store it in Beautiful Soup format
soup = BeautifulSoup(webpage, 
    features="lxml", 
    # from_encoding = [INSERT ENCODING IF KNOWN]
)
```

Here's a page from the docs also laying out the different parsers and their features, for further reference: [table of parsers](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser).

### Analysis

Digging through the resulting soup is easy enough with functions like ```soup.find_all()```, and checking if something is a tag or a NavigableString with ```isinstance()``` calls. There's reams of documentation and SO content to help me dig through and extract what I want.

The doc shows two ways to preview the soup or its selected sub-components: [pretty](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#pretty-printing) vs. [non-pretty printing](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#non-pretty-printing). I tend to prefer pretty printing because it's nicer and well-indented. All you need to use is the ```print(soup.prettify())``` call. 

Another thing worth remembering is that all HTML pages have [some sort of text encoding]({{< ref "/posts/2023-05-18-character-encodings" >}}) they are written in (e.g. ASCII, UTF-8). [BS4 will convert everything I read regardless of its encoding into the Unicode character set](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#encodings). 

Very occasionally, there might be errors in how certain characters are processed, because BS4 does the conversion based on guesswork about the original encoding of the webpage, using another library called _Unicode, Dammit_. These are usually revealed as I navigate the contents and tags of the page. If that happens and you uncover it during analysis, it would be worth going back and using the ```from_encoding``` parameter for the ```BeautifulSoup()``` function when parsing the scraped webpage. If you donâ€™t know what the correct encoding is, but I know that _Unicode, Dammit_ is guessing wrong, I can pass the wrong guesses in as a list to the```exclude_encodings``` argument.


### File Output Considerations

Once it's time to write out the soup for later analysis/revisiting, instead of say conducting all my analysis ad-hoc over a thousand scraped pages and deleting the scraped HTML once I think I'm done because there's no way I might have made an error and could have to re-scrape this in three weeks when I uncover said error, there's a couple different ways to go about it.

One thing to know is that BS4 outputs a UTF-8 encoded document, unless specified otherwise. I have never run into errors with this or have had to use an alternate encoding version for output, but just in case, here's a handy link to navigate those issues: [docs page for output encodings](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output-encoding).

#### HTML files and Encoding Considerations

A simple way: save as an HTML file!

```python
with open("filepath/scraped_page.html", "w", encoding='utf-8') as file:
    file.write(str(soup))
```

Note: writing mode for this must be text (```w```), not bytes mode. This is also because you explicitly convert the soup to str before writing. More on the distinction of text vs. bytes [here]({{< ref "/content/posts/2023-05-23-Reading-Writing-Modifiers.md" >}}).

Then, when it's time to read it back in:

```python
with open("filepath/scraped_page.html", "rb") as file:
    soup = BeautifulSoup(file.read(), "lxml")
```

This is good even when a webpage is extremely neted