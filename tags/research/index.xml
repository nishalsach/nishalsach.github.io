<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research on Sachita Nishal</title>
    <link>https://nishalsach.github.io/tags/research/</link>
    <description>Recent content in research on Sachita Nishal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://nishalsach.github.io/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning to scope (and also breathe)</title>
      <link>https://nishalsach.github.io/posts/2025-04-15-post-prospectus/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2025-04-15-post-prospectus/</guid>
      <description>I recently defended my prospectus!
I&amp;rsquo;ve spent the last few years of my PhD focused on different problems around value-sensitive design for journalism and science communication. Mostly I&amp;rsquo;ve engaged in artefact design to support tasks like newsgathering and sensemaking that science journalists engage in on a daily basis. When put together, I believe my work makes interesting contributions to how journalists generate the newsworthiness of the stories they ultimately tell the public.</description>
    </item>
    
    <item>
      <title>Recent write-up on LLMs and uncertainty</title>
      <link>https://nishalsach.github.io/posts/2025-04-02-lab-exercise/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2025-04-02-lab-exercise/</guid>
      <description>This quarter, the Computational Journalism lab undertook a little science comunication exercise: each of us wrote a short blogpost dissecting a recent research paper + its implications for news practitioners. Here&amp;rsquo;s the link to my post, published on the Generative AI in the Newsroom (GAIN) blog.</description>
    </item>
    
    <item>
      <title>A prototype newsgathering tool for science journalists</title>
      <link>https://nishalsach.github.io/posts/2024-11-01-cplusj-blog/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2024-11-01-cplusj-blog/</guid>
      <description>This past month, I presented our recent work on using LLMs to aid science journalists in reading complex scientific papers. Here&amp;rsquo;s the link to my blog post about it, published on the Generative AI in the Newsroom blog.</description>
    </item>
    
    <item>
      <title>Talk &#43; Responses: Blueprints for Evaluating AI in Journalism</title>
      <link>https://nishalsach.github.io/posts/2024-05-02-ai-evals-aspen-talk/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2024-05-02-ai-evals-aspen-talk/</guid>
      <description>Yesterday I delivered a short talk at Aspen Digital&amp;rsquo;s event called AI &amp;amp; News: Charting the Course. It was hosted at the Ford Foundation Center for Social Justice in NYC, which is a really pretty venue. They have so many trees inside! And also apprently: birds1! Here&amp;rsquo;s the slide deck for this talk, made under a certain degree of sleep deprivation-based duress and fever brain.
Pretty trees! This was a really fun time for me for a bunch of reasons.</description>
    </item>
    
    <item>
      <title>Talk &#43; Responses: Gender Biases in LLM Narratives</title>
      <link>https://nishalsach.github.io/posts/2024-04-02-story-discovery-at-scale/</link>
      <pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2024-04-02-story-discovery-at-scale/</guid>
      <description>Last week I delivered a short talk at Big Local News&amp;rsquo; conference Story Discovery at Scale 2024. It was great to be back this year &amp;ndash; 2023 was my first time attending this event, and meeting the groups around Big Local News! I felt like I was going back to a community of people that I now know and understand a little better, and it was nice to have some familiar faces to say hello to and catch up with.</description>
    </item>
    
    <item>
      <title>Two Years into the PhD: Thoughts and Half-thoughts</title>
      <link>https://nishalsach.github.io/posts/2022-10-27-evaluating-my-phd/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2022-10-27-evaluating-my-phd/</guid>
      <description>One thing I did not (really, truly, practically) take into consideration when I started doing the PhD was how difficult it would be. I learnt soon enough, of course. But you know that joke &amp;ndash; that the best time to be in a PhD program is the summer before it begins, when you impress everyone by telling them you&amp;rsquo;re going to get a PhD without having to actually do anything for it?</description>
    </item>
    
    <item>
      <title>A Short Primer on Mutual Information</title>
      <link>https://nishalsach.github.io/posts/2022-04-25-short-primer_mi/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2022-04-25-short-primer_mi/</guid>
      <description>Definitions Conceptual Definition: MI is the reduction in uncertainty when predicting one variable (&amp;ldquo;outcome&amp;rdquo;) in a system, if we know the value of another variable in the system. It will always be &amp;gt;=0, and higher values mean higher dependency between the two variables Formula (for discrete processes/variables X1 and X2): As must be apparent, for two independent variables, MI = 0. Eg. Two coins are tossed; the results of one coin do not help us measure the results of the other in any way Another way to look at this (and I still have some trouble wrapping my head around this) is that MI is the KL Divergence in going from the joint distribution P(X1, X2) to the product of the marginal distributions P(X1).</description>
    </item>
    
    <item>
      <title>My Thesis Experience at Northwestern University</title>
      <link>https://nishalsach.github.io/posts/2020-06-29-thesis_northwestern/</link>
      <pubDate>Mon, 29 Jun 2020 13:39:12 +0530</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2020-06-29-thesis_northwestern/</guid>
      <description>In 2019, I completed my undergrad thesis at Northwestern University, where I worked in Dr. Luis Amaral&amp;rsquo;s lab to model complex networks of tropes in American films starting from the 1890s.
I wrote a brief account of how I applied for my thesis, as well as my takeaways from this momentous experience, and it was published on BITS Goa&amp;rsquo;s blog for undergrad research experiences. Here&amp;rsquo;s a link to the post: Sachita Nishalâ€™s Thesis at Northwestern University!</description>
    </item>
    
    <item>
      <title>Lessons From a Massive(ly-Annoying) Scraping Project in Python</title>
      <link>https://nishalsach.github.io/posts/2019-09-16-lessons-from-a-massively-annoying-scraping-project-in-python/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2019-09-16-lessons-from-a-massively-annoying-scraping-project-in-python/</guid>
      <description>Hello!
I&amp;rsquo;m honestly quite unsure of what SEO miracle led you here, given the enthusiastically descriptive, but less than optimal title for this post. I&amp;rsquo;m further unsure of why you actually clicked this link when it showed up. But does that make me any less grateful that you&amp;rsquo;re here? Certainly not!
I wrote this post to document my experiences while scraping a massive crowdsourced website called TVTropes. During this venture, I spent many an afternoon quite desperately Googling &amp;ldquo;ugh web-scraping is so annoying save me python&amp;rdquo;.</description>
    </item>
    
  </channel>
</rss>
