<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Sachita Nishal</title>
    <link>https://nishalsach.github.io/tags/python/</link>
    <description>Recent content in python on Sachita Nishal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://nishalsach.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading and Writing Files in Python: Modifiers</title>
      <link>https://nishalsach.github.io/posts/2023-05-23-reading-writing-modifiers/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2023-05-23-reading-writing-modifiers/</guid>
      <description>Different modes of reading and writing to files, and what they do:
r: Only read. This is the default if no mode argument is given. w: Only write. An existing file with the same name will be over-written. a: Append data to the end of the file. Still writing. r+: Both reading and writing and can be done. I never use this.
From the Python Docs:
Normally, files are opened in text mode, that means, you read and write strings from and to the file, which are encoded in a specific encoding.</description>
    </item>
    
    <item>
      <title>BeautifulSoup4Dummies</title>
      <link>https://nishalsach.github.io/posts/2023-05-19-bs4dummies/</link>
      <pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2023-05-19-bs4dummies/</guid>
      <description>Note: All links are from BS v4.12.0 documentation and reference, but the gist should probably hold for a while.
BeautifulSoup4 + Python is a formidable combo for web-scraping when the territory I&amp;rsquo;m scraping in is uncertain, webpages are static, and I need to spin up something quick and dirty so I can hunt around through the HTML to specifically extract what I need.
Another thing is: when pulling web data with BS4, sometimes it&amp;rsquo;s really handy to keep separate files of the subset/tags/text in a page that are of interest, but then also save original HTML files just to have them on hand in case extraction does not go according to plan.</description>
    </item>
    
    <item>
      <title>Debugging Inside Scikit-Learn Pipelines</title>
      <link>https://nishalsach.github.io/posts/2021-08-17-debugging-sklearn-pipelines/</link>
      <pubDate>Tue, 17 Aug 2021 18:01:26 +0530</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2021-08-17-debugging-sklearn-pipelines/</guid>
      <description>Click the given link to skip to the code and spare yourself my tortuous introduction: code for sklearn Pipeline debugging Transformer. Don&amp;rsquo;t click the link if you&amp;rsquo;re debugging at 2pm on a slow Thursday and want something, anything, to distract you from staring at VSCode and its quickly blurring lines of code. I can help you pretend you&amp;rsquo;re working as you scan the next few paragraphs of my inner monologue.</description>
    </item>
    
    <item>
      <title>Lessons From a Massive(ly-Annoying) Scraping Project in Python</title>
      <link>https://nishalsach.github.io/posts/2019-09-16-lessons-from-a-massively-annoying-scraping-project-in-python/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2019-09-16-lessons-from-a-massively-annoying-scraping-project-in-python/</guid>
      <description>Hello!
I&amp;rsquo;m honestly quite unsure of what SEO miracle led you here, given the enthusiastically descriptive, but less than optimal title for this post. I&amp;rsquo;m further unsure of why you actually clicked this link when it showed up. But does that make me any less grateful that you&amp;rsquo;re here? Certainly not!
I wrote this post to document my experiences while scraping a massive crowdsourced website called TVTropes. During this venture, I spent many an afternoon quite desperately Googling &amp;ldquo;ugh web-scraping is so annoying save me python&amp;rdquo;.</description>
    </item>
    
  </channel>
</rss>
