<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Sachita Nishal</title>
    <link>https://nishalsach.github.io/tags/statistics/</link>
    <description>Recent content in statistics on Sachita Nishal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Apr 2022 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://nishalsach.github.io/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Short Primer on Mutual Information</title>
      <link>https://nishalsach.github.io/posts/2022-04-25-short-primer_mi/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://nishalsach.github.io/posts/2022-04-25-short-primer_mi/</guid>
      <description>Definitions  Conceptual Definition: MI is the reduction in uncertainty when predicting one variable (&amp;ldquo;outcome&amp;rdquo;) in a system, if we know the value of another variable in the system.  It will always be &amp;gt;=0, and higher values mean higher dependency between the two variables   Formula (for discrete processes/variables X1 and X2):   As must be apparent, for two independent variables, MI = 0. Eg. Two coins are tossed; the results of one coin do not help us measure the results of the other in any way Another way to look at this (and I still have some trouble wrapping my head around this) is that MI is the KL Divergence in going from the joint distribution P(X1, X2) to the product of the marginal distributions P(X1).</description>
    </item>
    
  </channel>
</rss>